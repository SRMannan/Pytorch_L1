{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVa4CE3LwHb2WLfAt/q3Fp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SRMannan/Pytorch_L1/blob/main/3_AutoGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoSvY2Ki1WHG",
        "outputId": "c18302e7-753a-42a1-f106-011c95498bdb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4zsjSZ00jfj",
        "outputId": "dab74b5e-aefe-4db4-bdca-070dc2105a10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#AUTOGRAD NEEDED IN ORDER TO ENSURE THAT THE OPS OF DIFF ARE EASILY COMPUTETED\n",
        "\n",
        "x = torch.tensor(3.0 , requires_grad = True)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWwHFl5i1qVW",
        "outputId": "4f685cc7-89e7-474c-c9a7-82de91e292b8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "-rNM8ep92L2S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxwh5wcM2mBg",
        "outputId": "fafc147e-15fd-4777-e042-e98cda2b62e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.sin(y)\n"
      ],
      "metadata": {
        "id": "r07nXotU2Hd7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdxVuM392YUO",
        "outputId": "3886821d-d0a6-45aa-c0d3-13143804462c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj9Natrh2Yf5",
        "outputId": "585c92e2-ce6e-4b6d-8c71-acabf23531ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv_gMmrj2yu6",
        "outputId": "159aaf61-1776-4bf3-e013-38edb574299f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4121, grad_fn=<SinBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "mS_p9qem2yxQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "topAdtEu2y0x",
        "outputId": "af05c54a-13e0-4e7d-e730-73c458a71cec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-5.4668)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LETS DO SOME CALCULATIONS USING AUTOGRAD"
      ],
      "metadata": {
        "id": "grzCqMrL4zsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "kgVboB0b5GjA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(5.6 , device = device , requires_grad = True)\n",
        "y = torch.tensor(0.0 , requires_grad = True)"
      ],
      "metadata": {
        "id": "wFOReydy4zV_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(1.0 , requires_grad = True)\n",
        "b = torch.tensor(0.0 , requires_grad = True)"
      ],
      "metadata": {
        "id": "JRuzSfSj5ZtB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = w * x + b\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6M-QWXY4zJm",
        "outputId": "f343eeea-e768-4c4c-f3ee-0fcde6f3cce2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.6000, device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.sigmoid(y)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5B-nBsX4zGM",
        "outputId": "d355799c-5008-4382-951a-add1d6afd9ae"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9963, device='cuda:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearing gradients in PyTorch's autograd system is necessary to prevent gradient accumulation, which ensures that gradients from previous iterations do not interfere with the current iteration's computation"
      ],
      "metadata": {
        "id": "9kL5v8__8WAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#clearing grad\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "r3K_HzPX4zD2",
        "outputId": "1459b642-7b89-4e85-d3e3-4249f2feebf1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'zero_'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-08a94a9fcc84>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#clearing grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, stopping gradient tracking is often necessary when you do not want to compute or store gradients for a specific computation. This is particularly useful during inference, fine-tuning, or when working with pretrained models where you want to freeze some layers."
      ],
      "metadata": {
        "id": "O6HhyEs29D9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#appr 1\n",
        "x.requires_grad_(False)\n",
        "\n",
        "#appr2\n",
        "with torch.no_grad():\n",
        "  y = x**2"
      ],
      "metadata": {
        "id": "yp1XdYue9Jbv"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}
